{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/d3yyeRH+Xi3umCmkbHbW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MehrdadJalali-KIT/BlackHole/blob/main/BlackHole_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2UlbUrED5Eg",
        "outputId": "03d9e265-19b8-4211-e510-f99d8bbebb18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "# Change working path\n",
        "os.chdir('/content/drive/MyDrive/Research/MOF/Black_Hole')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mqkBwMjECRf",
        "outputId": "ccb534f3-4a23-4265-87ea-548d1b2607b6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2024.3.5-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n",
            "Downloading rdkit-2024.3.5-cp310-cp310-manylinux_2_28_x86_64.whl (33.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2024.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "from sklearn.metrics import confusion_matrix, cohen_kappa_score, matthews_corrcoef, roc_auc_score\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from tensorflow.keras.models import load_model\n",
        "import warnings\n",
        "from rdkit import RDLogger\n",
        "from tensorflow.keras import models, layers, regularizers\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Suppress specific deprecation warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "# Additionally, suppress RDKit warnings globally\n",
        "RDLogger.DisableLog('rdApp.*')\n",
        "\n",
        "def generate_fingerprint(smiles):\n",
        "    \"\"\"Generates a molecular fingerprint given a SMILES string.\"\"\"\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is None:\n",
        "            return np.zeros((1024,), dtype=float)  # Return an array of zeros if molecule can't be parsed\n",
        "        return np.array(AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024), dtype=float)\n",
        "    except Exception as e:\n",
        "        print(f\"SMILES Parse Error: {e}\")\n",
        "        return np.zeros((1024,), dtype=float)  # Return an array of zeros in case of an error\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes):\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Calculate percentage accuracy for each element in the confusion matrix\n",
        "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "\n",
        "    # Combine counts and percentages for display\n",
        "    annot = np.empty_like(cm).astype(str)\n",
        "    nrows, ncols = cm.shape\n",
        "    for i in range(nrows):\n",
        "        for j in range(ncols):\n",
        "            c = cm[i, j]\n",
        "            p = cm_percentage[i, j]\n",
        "            annot[i, j] = f'{c}\\n({p:.1f}%)'  # Count and percentage\n",
        "\n",
        "    # Plot the confusion matrix with annotations\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=annot, fmt='', cmap='Blues', xticklabels=classes, yticklabels=classes, cbar=False)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.show()\n",
        "\n",
        "def label_encode_metal_names(metal_names):\n",
        "    \"\"\"Encodes metal names as integers.\"\"\"\n",
        "    metal_dict = {metal: idx for idx, metal in enumerate(np.unique(metal_names))}\n",
        "    return np.array([metal_dict[metal] for metal in metal_names])\n",
        "\n",
        "def preprocess_graph(graph, features):\n",
        "    # Determine the dimensionality of the feature vectors\n",
        "    feature_dimension = features.shape[1]\n",
        "\n",
        "    # Convert the graph to an adjacency matrix\n",
        "    adjacency_matrix = nx.adjacency_matrix(graph).toarray()\n",
        "\n",
        "    # Initialize an empty list to store feature vectors\n",
        "    feature_vectors = []\n",
        "\n",
        "    # Create a mapping from node labels to integer indices\n",
        "    node_to_index = {node: index for index, node in enumerate(graph.nodes())}\n",
        "\n",
        "    # Iterate over nodes in the graph\n",
        "    for node in graph.nodes():\n",
        "        # Get the integer index corresponding to the node label\n",
        "        node_index = node_to_index[node]\n",
        "        # Check if the node index is valid\n",
        "        if node_index < len(features):\n",
        "            # Append the feature vector corresponding to the node index\n",
        "            feature_vectors.append(features[node_index])\n",
        "        else:\n",
        "            # If the node index is out of range, assign a default feature vector\n",
        "            feature_vectors.append(np.zeros((feature_dimension,)))\n",
        "\n",
        "    # Convert the list of feature vectors to a numpy array\n",
        "    feature_matrix = np.array(feature_vectors)\n",
        "\n",
        "    return adjacency_matrix, feature_matrix\n",
        "\n",
        "def build_gcn_model(input_shape_feature, input_shape_adjacency, num_classes):\n",
        "    # Define input layers\n",
        "    x_inp_feature = Input(shape=(input_shape_feature,))\n",
        "    x_inp_adjacency = Input(shape=(input_shape_adjacency,))\n",
        "\n",
        "    # Feature processing with multiple layers\n",
        "    x_feature = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(x_inp_feature)\n",
        "    x_feature = Dropout(0.5)(x_feature)\n",
        "    x_feature = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x_feature)\n",
        "    x_feature = Dropout(0.3)(x_feature)\n",
        "\n",
        "    # Adjacency processing with multiple layers\n",
        "    x_adjacency = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(x_inp_adjacency)\n",
        "    x_adjacency = Dropout(0.5)(x_adjacency)\n",
        "    x_adjacency = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x_adjacency)\n",
        "    x_adjacency = Dropout(0.3)(x_adjacency)\n",
        "\n",
        "    # Concatenate feature and adjacency outputs\n",
        "    x = concatenate([x_feature, x_adjacency])\n",
        "\n",
        "    # Output layer\n",
        "    output = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs=[x_inp_feature, x_inp_adjacency], outputs=output)\n",
        "\n",
        "    # Using a smaller learning rate\n",
        "    optimizer = Adam(learning_rate=0.0009)\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(optimizer=optimizer, loss=CategoricalCrossentropy(), metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_feedforward_model(input_shape, num_classes):\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(input_shape,)),\n",
        "        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "def train_gcn_model(model, adjacency_matrix, feature_matrix, labels, epochs, batch_size):\n",
        "    if model is not None and adjacency_matrix is not None and feature_matrix is not None and labels is not None:\n",
        "        # Early stopping to prevent overfitting\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "        # ModelCheckpoint to save the best model\n",
        "        model_checkpoint = ModelCheckpoint('best_gcn_model.keras', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "\n",
        "        start_time = time.time()\n",
        "        # Train the model\n",
        "        history = model.fit([feature_matrix, adjacency_matrix], labels,\n",
        "                            epochs=epochs, batch_size=batch_size,\n",
        "                            validation_split=0.2, callbacks=[early_stopping, model_checkpoint])\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Calculate total training time\n",
        "        total_training_time = end_time - start_time\n",
        "        print(f\"Total training time: {total_training_time:.2f} seconds\")\n",
        "\n",
        "        return history\n",
        "    else:\n",
        "        print(\"Error: One or more input arguments to train_gcn_model is None.\")\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    edge_list_filenames = [\n",
        "        'sparsified_graph_edges_blackhole_0.1.csv',\n",
        "        'sparsified_graph_edges_blackhole_0.2.csv',\n",
        "        'sparsified_graph_edges_blackhole_0.3.csv',\n",
        "        'sparsified_graph_edges_blackhole_0.4.csv',\n",
        "        'sparsified_graph_edges_blackhole_0.5.csv',\n",
        "        'sparsified_graph_edges_blackhole_0.6.csv',\n",
        "        'sparsified_graph_edges_blackhole_0.7.csv',\n",
        "        'sparsified_graph_edges_blackhole_0.8.csv',\n",
        "        'sparsified_graph_edges_blackhole_0.9.csv'\n",
        "    ]\n",
        "\n",
        "    summary_data_filename = '1M1L3D_summary.csv'\n",
        "\n",
        "    # Initialize lists to track accuracies and thresholds\n",
        "    accuracies = []\n",
        "    thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "\n",
        "    # Loop through all edge list files\n",
        "    for edges_list_filename, threshold in zip(edge_list_filenames, thresholds):\n",
        "        print(f\"Processing edge list file: {edges_list_filename}\")\n",
        "\n",
        "        # Load data\n",
        "        edges_list = pd.read_csv(edges_list_filename, header=None, names=['source', 'target', 'weight'], delimiter=' ')\n",
        "        summary_data = pd.read_csv(summary_data_filename)\n",
        "\n",
        "        node_labels_source = edges_list['source'].astype(str).unique()\n",
        "        node_labels_target = edges_list['target'].astype(str).unique()\n",
        "        node_labels = np.unique(np.concatenate((node_labels_source, node_labels_target)))\n",
        "        node_labels = list(set(node_labels))\n",
        "\n",
        "        print(\"Unique node labels:\", len(node_labels))\n",
        "\n",
        "        summary_data_filtered = summary_data[summary_data['refcode'].isin(node_labels)]\n",
        "        print(\"Filtered summary data:\\n\", len(summary_data_filtered))\n",
        "\n",
        "        if not summary_data_filtered.empty:\n",
        "            linker_smiles = summary_data_filtered['linker SMILES']\n",
        "            if not linker_smiles.empty:\n",
        "                # Generate features\n",
        "                linker_features = np.stack(linker_smiles.dropna().apply(generate_fingerprint).values)\n",
        "                metal_names = summary_data_filtered['metal']\n",
        "                metal_features = label_encode_metal_names(metal_names).reshape(-1, 1)\n",
        "\n",
        "                other_features = summary_data_filtered[['Largest Cavity Diameter', 'Largest Free Sphere']].values.astype('float32')\n",
        "                features = np.concatenate((linker_features, metal_features, other_features), axis=1)\n",
        "\n",
        "                # Generate labels\n",
        "                summary_data_filtered['PLD_category'] = pd.cut(\n",
        "                    summary_data_filtered['Pore Limiting Diameter'],\n",
        "                    bins=[-np.inf, 2.4, 4.4, 5.9, np.inf],\n",
        "                    labels=['nonporous', 'small pore', 'medium pore', 'large pore']\n",
        "                )\n",
        "                labels = pd.get_dummies(summary_data_filtered['PLD_category']).values\n",
        "\n",
        "                # Split the data into training and testing sets\n",
        "                X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=56)\n",
        "\n",
        "                # Load the sparsified graph\n",
        "                graph = nx.read_weighted_edgelist(edges_list_filename)\n",
        "\n",
        "                # Preprocess the graph data\n",
        "                adjacency_matrix, feature_matrix = preprocess_graph(graph, features)\n",
        "\n",
        "                # Split the adjacency and feature matrices accordingly\n",
        "                adj_train, adj_test, feat_train, feat_test = train_test_split(adjacency_matrix, feature_matrix, test_size=0.2, random_state=56)\n",
        "\n",
        "                # Provide the number of classes\n",
        "                num_classes = labels.shape[1]\n",
        "\n",
        "                # Build the GCN model\n",
        "                gcn_model = build_gcn_model(feat_train.shape[1], adj_train.shape[1], num_classes)\n",
        "\n",
        "                # Train the GCN model\n",
        "                history = train_gcn_model(gcn_model, adj_train, feat_train, y_train, epochs=40, batch_size=32)\n",
        "\n",
        "                # Evaluate the model on the test set\n",
        "                test_loss, test_accuracy = gcn_model.evaluate([feat_test, adj_test], y_test, verbose=0)\n",
        "                print(f'Test Accuracy for threshold {threshold}: {test_accuracy}')\n",
        "\n",
        "                # Track the accuracy\n",
        "                accuracies.append(test_accuracy)\n",
        "\n",
        "                # Continue with your evaluation metrics and comparison logic\n",
        "                # ...\n",
        "            else:\n",
        "                print(\"Error: linker_smiles column is empty.\")\n",
        "        else:\n",
        "            print(\"Error: summary_data_filtered DataFrame is empty.\")\n",
        "\n",
        "    # Plot the accuracy comparison\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(thresholds, accuracies, marker='o', color='b', label='Test Accuracy')\n",
        "    plt.xlabel('Threshold')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('GCN Test Accuracy Across Different Sparsification Thresholds')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "FpoIPjwQEOJ1",
        "outputId": "cd300ae9-d34e-40ce-b936-e2ab476982f4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing edge list file: sparsified_graph_edges_blackhole_0.1.csv\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'sparsified_graph_edges_blackhole_0.1.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-5192077cab59>\u001b[0m in \u001b[0;36m<cell line: 184>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0medges_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medges_list_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'target'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0msummary_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_data_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sparsified_graph_edges_blackhole_0.1.csv'"
          ]
        }
      ]
    }
  ]
}