{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZvhhuxbytY5kd1+QP843R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MehrdadJalali-AI/BlackHole/blob/main/BlackHoleMethod_Updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odcl_UpPvQhn",
        "outputId": "ceb8b8d6-a1c7-4838-8411-1b3f93a22a84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "# Change working path\n",
        "os.chdir('/content/drive/MyDrive/Research/MOF/Black_Hole/Implementation/Data')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FI-lYOQvq9n",
        "outputId": "1d85e651-c310-4a38-df6b-6d64a38e4ac7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2024.3.6-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (11.0.0)\n",
            "Downloading rdkit-2024.3.6-cp310-cp310-manylinux_2_28_x86_64.whl (32.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2024.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")  # Suppress warnings\n",
        "from rdkit import RDLogger\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx.algorithms.community as nx_comm\n",
        "import time\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "# Suppress specific deprecation warnings and RDKit warnings globally\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "RDLogger.DisableLog('rdApp.*')\n",
        "\n",
        "def load_edges_list(filename):\n",
        "    \"\"\"Loads edge list from a CSV file.\"\"\"\n",
        "    edges_list = pd.read_csv(filename)\n",
        "    # Ensure the necessary columns exist\n",
        "    required_columns = {'source', 'target', 'weight'}\n",
        "    if not required_columns.issubset(edges_list.columns):\n",
        "        raise ValueError(f\"The edges list file must contain the columns: {required_columns}\")\n",
        "    return edges_list\n",
        "\n",
        "def load_summary_data(filename, node_labels):\n",
        "    \"\"\"Loads summary data and ensures alignment with node labels.\"\"\"\n",
        "    summary_data = pd.read_csv(filename)\n",
        "    # Ensure that 'refcode' is the column containing node labels\n",
        "    if 'refcode' in summary_data.columns:\n",
        "        summary_data.set_index('refcode', inplace=True)\n",
        "    else:\n",
        "        raise ValueError(\"The summary data file does not contain a 'refcode' column.\")\n",
        "    # Ensure the index aligns with node labels\n",
        "    summary_data = summary_data.loc[node_labels]\n",
        "    return summary_data\n",
        "\n",
        "def detect_communities(graph):\n",
        "    \"\"\"Detects communities in the graph using the Girvan-Newman algorithm.\"\"\"\n",
        "    communities_generator = nx_comm.girvan_newman(graph)\n",
        "    top_level_communities = next(communities_generator)\n",
        "    return [list(community) for community in top_level_communities]\n",
        "\n",
        "def assign_community_ids(communities):\n",
        "    \"\"\"Assigns a unique ID to each community.\"\"\"\n",
        "    community_ids = {}\n",
        "    for i, community in enumerate(communities):\n",
        "        for node in community:\n",
        "            community_ids[node] = i\n",
        "    return community_ids\n",
        "\n",
        "def calculate_gravity_per_community(graph, communities):\n",
        "    \"\"\"Calculates normalized gravity per community.\"\"\"\n",
        "    community_gravity = {}\n",
        "    degree_centrality = {}\n",
        "    betweenness_centrality = {}\n",
        "    edge_weight_sum = {}\n",
        "\n",
        "    for community in communities:\n",
        "        subgraph = graph.subgraph(community)\n",
        "        degree_centrality_community = nx.degree_centrality(subgraph)\n",
        "        betweenness_centrality_community = nx.betweenness_centrality(subgraph, normalized=True)\n",
        "        edge_weight_sum_community = {node: sum(data['weight'] for _, _, data in subgraph.edges(node, data=True)) for node in subgraph.nodes()}\n",
        "\n",
        "        # Convert values to arrays for normalization\n",
        "        degree_values = list(degree_centrality_community.values())\n",
        "        betweenness_values = list(betweenness_centrality_community.values())\n",
        "        weight_sum_values = list(edge_weight_sum_community.values())\n",
        "\n",
        "        # Normalize within the community\n",
        "        scaler = MinMaxScaler()\n",
        "        normalized_degree = scaler.fit_transform(np.array(degree_values).reshape(-1, 1)).flatten()\n",
        "        normalized_betweenness = scaler.fit_transform(np.array(betweenness_values).reshape(-1, 1)).flatten()\n",
        "        normalized_weight_sum = scaler.fit_transform(np.array(weight_sum_values).reshape(-1, 1)).flatten()\n",
        "\n",
        "        # Assign normalized values and calculate gravity for each node in the community\n",
        "        for idx, node in enumerate(community):\n",
        "            degree = normalized_degree[idx]\n",
        "            betweenness = normalized_betweenness[idx]\n",
        "            weight_sum = normalized_weight_sum[idx]\n",
        "\n",
        "            # Gravity is the average of the normalized metrics\n",
        "            gravity = (degree + betweenness + weight_sum) / 3\n",
        "            community_gravity[node] = gravity\n",
        "            degree_centrality[node] = degree_centrality_community[node]\n",
        "            betweenness_centrality[node] = betweenness_centrality_community[node]\n",
        "            edge_weight_sum[node] = edge_weight_sum_community[node]\n",
        "\n",
        "    return community_gravity, degree_centrality, betweenness_centrality, edge_weight_sum\n",
        "\n",
        "def black_hole_strategy_combined(graph, summary_data, gravity_scores, communities, threshold):\n",
        "    \"\"\"Combines gravity metrics with stratified sampling to preserve PLD distribution.\"\"\"\n",
        "    if threshold == 0.0:\n",
        "        return graph  # Return the original graph without any changes.\n",
        "\n",
        "    nodes_to_remove = []\n",
        "\n",
        "    for community in communities:\n",
        "        community_nodes = [node for node in community if node in graph.nodes()]\n",
        "        community_pld = summary_data.loc[community_nodes, 'Pore Limiting Diameter'].to_frame()\n",
        "        community_pld['Gravity'] = [gravity_scores.get(node, 0) for node in community_nodes]\n",
        "\n",
        "        # Determine the number of nodes to keep\n",
        "        num_nodes = len(community_nodes)\n",
        "        num_to_keep = int((1 - threshold) * num_nodes)\n",
        "\n",
        "        if num_to_keep <= 0:\n",
        "            # If threshold is too high, keep at least one node\n",
        "            num_to_keep = 1\n",
        "\n",
        "        # Create bins for PLD values\n",
        "        num_bins = min(10, num_nodes)  # Choose an appropriate number of bins\n",
        "        community_pld['PLD_bin'] = pd.qcut(community_pld['Pore Limiting Diameter'], q=num_bins, duplicates='drop')\n",
        "\n",
        "        # Initialize list to collect nodes to keep\n",
        "        nodes_to_keep = []\n",
        "\n",
        "        # Perform stratified sampling within bins, prioritizing nodes with higher gravity scores\n",
        "        for pld_bin, bin_group in community_pld.groupby('PLD_bin'):\n",
        "            bin_nodes = bin_group.index.tolist()\n",
        "            bin_size = len(bin_nodes)\n",
        "            bin_num_to_keep = max(1, int((num_to_keep / num_nodes) * bin_size))\n",
        "\n",
        "            # Sort nodes in the bin by gravity score in descending order\n",
        "            bin_group_sorted = bin_group.sort_values(by='Gravity', ascending=False)\n",
        "\n",
        "            # Select top nodes based on gravity scores\n",
        "            sampled_nodes = bin_group_sorted.head(bin_num_to_keep).index.tolist()\n",
        "            nodes_to_keep.extend(sampled_nodes)\n",
        "\n",
        "        # Identify nodes to remove\n",
        "        nodes_to_remove_in_community = set(community_nodes) - set(nodes_to_keep)\n",
        "        nodes_to_remove.extend(nodes_to_remove_in_community)\n",
        "\n",
        "    graph.remove_nodes_from(nodes_to_remove)\n",
        "    return graph\n",
        "\n",
        "def save_extended_node_features(graph, original_summary_filename, gravity, degree_centrality, betweenness_centrality, edge_weight_sum, community_ids, filename):\n",
        "    \"\"\"Saves extended node features including gravity metrics and community IDs.\"\"\"\n",
        "    remaining_nodes = list(graph.nodes())\n",
        "\n",
        "    # Load the original summary data without any modification\n",
        "    original_summary_data = pd.read_csv(original_summary_filename)\n",
        "    if 'refcode' in original_summary_data.columns:\n",
        "        original_summary_data.set_index('refcode', inplace=True)\n",
        "    else:\n",
        "        raise ValueError(\"The summary data file does not contain a 'refcode' column.\")\n",
        "\n",
        "    # Ensure we keep only the rows corresponding to the remaining nodes\n",
        "    original_summary_data = original_summary_data.loc[original_summary_data.index.isin(remaining_nodes)]\n",
        "\n",
        "    # Create a DataFrame for the new features\n",
        "    metrics_data = pd.DataFrame({\n",
        "        'refcode': remaining_nodes,\n",
        "        'Community ID': [community_ids.get(node, -1) for node in remaining_nodes],\n",
        "        'Gravity': [gravity.get(node, 0) for node in remaining_nodes],\n",
        "        'Degree Centrality': [degree_centrality.get(node, 0) for node in remaining_nodes],\n",
        "        'Betweenness Centrality': [betweenness_centrality.get(node, 0) for node in remaining_nodes],\n",
        "        'Edge Weight Sum': [edge_weight_sum.get(node, 0) for node in remaining_nodes],\n",
        "    })\n",
        "\n",
        "    # Set 'refcode' as the index for merging\n",
        "    metrics_data.set_index('refcode', inplace=True)\n",
        "\n",
        "    # Merge the original summary data with the new metrics data\n",
        "    final_data = pd.concat([original_summary_data, metrics_data], axis=1)\n",
        "    final_data.index.name = 'refcode'\n",
        "\n",
        "    # Save the result to the specified file\n",
        "    final_data.to_csv(filename, index=True)\n",
        "    print(f\"Extended features of the remaining nodes saved to {filename}\")\n",
        "\n",
        "def visualize_pld_distribution(summary_data, graph, title):\n",
        "    \"\"\"Visualizes the distribution of PLD values in the graph.\"\"\"\n",
        "    # Ensure that we are using the correct node labels to index the DataFrame\n",
        "    node_list = list(graph.nodes())\n",
        "\n",
        "    try:\n",
        "        pld_values = summary_data.loc[node_list, 'Pore Limiting Diameter']\n",
        "    except KeyError:\n",
        "        raise KeyError(\"Some node labels in the graph are not found in the summary_data DataFrame.\")\n",
        "\n",
        "    # Plotting the PLD distribution\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.hist(pld_values.dropna(), bins=50, color='g', alpha=0.7)  # Handle missing values\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Pore Limiting Diameter')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_time = time.time()\n",
        "    thresholds = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
        "    edges_list_filename = 'edges_list_0.8_Full_2.csv'\n",
        "    # edges_list_filename = 'edges_list_short.csv'\n",
        "    summary_data_filename = '1M1L3D_summary.csv'\n",
        "\n",
        "    # Load edge list and node labels\n",
        "    edges_list = load_edges_list(edges_list_filename)\n",
        "    node_labels = pd.concat([edges_list['source'], edges_list['target']]).unique()\n",
        "\n",
        "    # Load the summary data\n",
        "    summary_data = load_summary_data(summary_data_filename, node_labels)\n",
        "\n",
        "    # Construct the graph\n",
        "    graph = nx.Graph()\n",
        "    for _, row in edges_list.iterrows():\n",
        "        graph.add_edge(row['source'], row['target'], weight=row['weight'])\n",
        "\n",
        "    # Detect initial communities\n",
        "    initial_communities = detect_communities(graph)\n",
        "    community_ids = assign_community_ids(initial_communities)  # Get community IDs\n",
        "\n",
        "    # Get the initial number of nodes\n",
        "    initial_num_nodes = graph.number_of_nodes()\n",
        "    print(f\"Initial number of communities: {len(initial_communities)}\")\n",
        "    print(f\"Initial number of nodes: {initial_num_nodes}\")\n",
        "\n",
        "    # Lists to store results for plotting\n",
        "    nodes_remaining = []\n",
        "    percentage_reduction = []\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        print(f\"\\nProcessing for threshold: {threshold}\")\n",
        "\n",
        "        # Reset the graph for each threshold\n",
        "        graph = nx.Graph()\n",
        "        for _, row in edges_list.iterrows():\n",
        "            graph.add_edge(row['source'], row['target'], weight=row['weight'])\n",
        "\n",
        "        # Calculate gravity metrics\n",
        "        gravity_per_community, degree_centrality, betweenness_centrality, edge_weight_sum = calculate_gravity_per_community(graph, initial_communities)\n",
        "\n",
        "        # Number of nodes before applying the Black Hole Strategy at this threshold\n",
        "        num_nodes_before = graph.number_of_nodes()\n",
        "        print(f\"Number of nodes before applying Black Hole Strategy for threshold {threshold}: {num_nodes_before}\")\n",
        "\n",
        "        # Apply the Black Hole Strategy combining gravity and stratified sampling\n",
        "        graph = black_hole_strategy_combined(graph, summary_data, gravity_per_community, initial_communities, threshold)\n",
        "\n",
        "        # Number of nodes after applying the Black Hole Strategy\n",
        "        num_nodes_after = graph.number_of_nodes()\n",
        "        print(f\"Number of nodes after applying Black Hole Strategy with threshold {threshold}: {num_nodes_after}\")\n",
        "\n",
        "        # Store the number of remaining nodes and percentage reduction\n",
        "        nodes_remaining.append(num_nodes_after)\n",
        "        percentage_reduction.append(((initial_num_nodes - num_nodes_after) / initial_num_nodes) * 100)\n",
        "\n",
        "        # Save the results\n",
        "        output_edges_filename = f'Edges_blackhole_{threshold}.csv'\n",
        "        output_features_filename = f'Node_features_blackhole_{threshold}.csv'\n",
        "        nx.write_edgelist(graph, output_edges_filename, data=['weight'])\n",
        "\n",
        "        # Save extended node features with community IDs included\n",
        "        save_extended_node_features(\n",
        "            graph, summary_data_filename, gravity_per_community, degree_centrality,\n",
        "            betweenness_centrality, edge_weight_sum, community_ids, output_features_filename\n",
        "        )\n",
        "\n",
        "        # Visualize PLD distribution\n",
        "        visualize_pld_distribution(summary_data, graph, title=f'PLD Distribution with Threshold {threshold}')\n",
        "\n",
        "    # Plot Threshold vs Number of Remaining Nodes\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(thresholds, nodes_remaining, marker='o', color='b')\n",
        "    plt.title(\"Threshold vs. Number of Remaining Nodes\")\n",
        "    plt.xlabel(\"Threshold\")\n",
        "    plt.ylabel(\"Number of Remaining Nodes\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot Threshold vs Percentage Reduction in Nodes\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(thresholds, percentage_reduction, marker='o', color='r')\n",
        "    plt.title(\"Threshold vs. Percentage Reduction in Nodes\")\n",
        "    plt.xlabel(\"Threshold\")\n",
        "    plt.ylabel(\"Percentage Reduction (%)\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Total runtime: {time.time() - start_time} seconds\")\n"
      ],
      "metadata": {
        "id": "X5Qg4YtDvmjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Black Hole Strategy for MOF Graphs\n",
        "\n",
        "In this notebook, we are implementing a **Black Hole Strategy** to refine the graph representation of Metal-Organic Frameworks (MOFs). This approach helps us reduce the complexity of the graph by identifying and removing less influential nodes (MOFs) based on a calculated \"gravity\" metric. The steps involved in this strategy are as follows:\n",
        "\n",
        "## 1. Loading the Edge List and Summary Data\n",
        "We begin by loading the edge list, which defines the relationships (edges) between MOFs (nodes), and the summary data, which contains additional features about the MOFs (such as chemical properties).\n",
        "\n",
        "- **Edge List**: Contains source and target nodes (MOFs) and their connection weights.\n",
        "- **Summary Data**: Contains features for each MOF, such as SMILES strings, metal types, and geometric properties like cavity diameter and pore size.\n",
        "\n",
        "## 2. Constructing the MOF Graph\n",
        "Using the edge list, we create a graph where each node represents a MOF, and edges represent the relationships between them. This graph allows us to explore the structure and connectivity of the MOFs.\n",
        "\n",
        "## 3. Detecting Communities\n",
        "To organize the graph, we apply **community detection**. Communities represent groups of MOFs that are more strongly connected to each other than to other parts of the graph.\n",
        "\n",
        "- **Girvan-Newman Algorithm**: This algorithm is used to detect communities by iteratively removing edges with high betweenness centrality, splitting the graph into smaller groups.\n",
        "\n",
        "## 4. Calculating the Gravity Metric\n",
        "Within each detected community, we calculate a **gravity metric** for each node. This metric combines two centrality measures:\n",
        "- **Degree Centrality**: Represents how many connections (edges) a node has.\n",
        "- **Betweenness Centrality**: Represents how often a node acts as a bridge between other nodes in the graph.\n",
        "\n",
        "The gravity of a node is the sum of its degree and betweenness centrality. Nodes with higher gravity are considered more influential within their communities.\n",
        "\n",
        "## 5. Applying the Black Hole Strategy\n",
        "In the Black Hole Strategy, we set a gravity threshold. Nodes (MOFs) with gravity below this threshold are considered less influential and are removed from the graph, along with their associated edges. This results in a \"sparsified\" graph containing only the most important nodes (black holes) within each community.\n",
        "\n",
        "## 6. Saving the Sparsified Graph and Features\n",
        "After removing low-gravity nodes, we save the new edge list of the sparsified graph and the features of the remaining nodes for further analysis. This refined dataset focuses on the most promising MOFs and simplifies future computations.\n",
        "\n",
        "## 7. Visualizing the Graph\n",
        "Finally, we visualize the sparsified graph to observe the structure after applying the Black Hole Strategy. The visualization helps us see how the graph has been reduced in complexity and highlights the key communities and their central nodes.\n",
        "\n",
        "## Benefits of the Black Hole Strategy:\n",
        "- **Efficiency**: By focusing on the most influential MOFs, we reduce the dataset size and computational cost for further analysis.\n",
        "- **Clarity**: The resulting graph highlights the most important MOFs, making it easier to identify key candidates for specific applications.\n"
      ],
      "metadata": {
        "id": "6e5HC4-gxQVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Influential Node Detection in Network Communities\n",
        "\n",
        "To identify the most influential nodes within each community in a network, we employ a comprehensive strategy that combines several centrality measures with the connectivity strength of each node. This approach helps in pinpointing nodes that not only hold strategic positions but are also highly connected, thereby playing a crucial role in the structure and dynamics of their communities.\n",
        "\n",
        "### Methodology\n",
        "\n",
        "1. **Degree Centrality**: Measures the number of direct connections a node has. Nodes with a high degree centrality can influence their immediate neighbors more significantly.\n",
        "\n",
        "2. **Betweenness Centrality**: Captures the extent to which a node lies on the shortest path between other nodes in the network. Nodes with high betweenness centrality can control the flow of information within the network, acting as critical conduits between different parts of the network.\n",
        "\n",
        "3. **Edge Weight Sum**: Reflects the total weight of all edges connected to a node, emphasizing the 'strength' or 'value' of its connections.\n",
        "\n",
        "### Normalization Strategy\n",
        "\n",
        "Each of the above metrics is normalized by their maximum values in the subgraph of each community to ensure a fair comparison across diverse network scales and topologies. The final 'gravity' score of each node is then calculated as the sum of these normalized values, providing a single metric to assess the relative influence of nodes within their respective communities.\n"
      ],
      "metadata": {
        "id": "lnxLRie0xRsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Black Hole Strategy for Graph Sparsification\n",
        "\n",
        "The **Black Hole Strategy** is a graph sparsification technique that progressively removes less important nodes from the graph while retaining the structurally crucial ones. The importance of nodes is quantified by a \"gravity\" score, which is computed based on multiple centrality metrics. This method ensures that the most central and influential nodes within each community are preserved, while the less connected or influential nodes are removed.\n",
        "\n",
        "## 1. Community Detection\n",
        "\n",
        "The first step in the Black Hole Strategy is to partition the graph into disjoint communities. This is done using the Girvan-Newman algorithm, which iteratively removes the edges with the highest betweenness centrality until communities are formed. Each community represents a subgraph of the original graph.\n",
        "\n",
        "Mathematically, the graph \\( G \\) is divided into communities \\( C_1, C_2, \\dots, C_k \\), where each community \\( C_i \\) is a subgraph of \\( G \\).\n",
        "\n",
        "\\[\n",
        "C = \\{C_1, C_2, \\dots, C_k\\}, \\quad \\text{where} \\ C_i \\subseteq G\n",
        "\\]\n",
        "\n",
        "## 2. Gravity Calculation\n",
        "\n",
        "For each node \\( v \\) in a community \\( C_i \\), three centrality metrics are calculated to determine its influence within the community:\n",
        "\n",
        "- **Degree Centrality** \\( d(v) \\): This measures the number of direct connections a node has, normalized by the number of possible connections.\n",
        "  \n",
        "  \\[\n",
        "  d(v) = \\frac{\\text{deg}(v)}{|C_i| - 1}\n",
        "  \\]\n",
        "\n",
        "- **Betweenness Centrality** \\( b(v) \\): This measures how often a node lies on the shortest paths between other nodes in the community.\n",
        "  \n",
        "  \\[\n",
        "  b(v) = \\frac{ \\sum_{s,t \\in C_i} \\sigma_{st}(v)}{\\sigma_{st}}, \\quad \\text{where} \\ \\sigma_{st} \\ \\text{is the number of shortest paths from} \\ s \\ \\text{to} \\ t\n",
        "  \\]\n",
        "\n",
        "- **Edge Weight Sum** \\( w(v) \\): This metric captures the total weight of edges connected to the node.\n",
        "\n",
        "  \\[\n",
        "  w(v) = \\sum_{(v, u) \\in E_i} \\text{weight}(v, u), \\quad \\text{where} \\ E_i \\ \\text{is the set of edges in community} \\ C_i\n",
        "  \\]\n",
        "\n",
        "These three metrics are normalized within each community using **Min-Max Normalization**. The gravity \\( g(v) \\) of a node is then calculated as the average of the normalized centrality scores:\n",
        "\n",
        "\\[\n",
        "g(v) = \\frac{d'(v) + b'(v) + w'(v)}{3}\n",
        "\\]\n",
        "\n",
        "Where \\( d'(v), b'(v), \\text{and } w'(v) \\) are the normalized degree, betweenness, and weight sum centralities, respectively.\n",
        "\n",
        "## 3. Node Removal Based on Gravity\n",
        "\n",
        "The Black Hole Strategy iteratively removes nodes based on their gravity score. The user defines a threshold \\( T \\) that controls the proportion of nodes to retain in each community. For example:\n",
        "- A threshold of \\( T = 0.1 \\) retains the top 90% of nodes (with the highest gravity) and removes the remaining 10%.\n",
        "- A threshold of \\( T = 0.2 \\) retains the top 80% of nodes, and so on.\n",
        "\n",
        "For each community \\( C_i \\), the nodes are sorted by their gravity scores in descending order, and the number of nodes to keep is calculated as:\n",
        "\n",
        "\\[\n",
        "\\text{Nodes to Keep} = \\lceil (1 - T) \\times |C_i| \\rceil\n",
        "\\]\n",
        "\n",
        "Nodes with the lowest gravity are removed, and the graph is updated by removing both the nodes and their associated edges.\n",
        "\n",
        "## 4. Graph Update\n",
        "\n",
        "After node removal, the graph \\( G \\) is updated to reflect the remaining nodes and their edges. The sparsified graph consists of nodes with the highest gravity, ensuring that the structural integrity and centrality of the graph are preserved while reducing its complexity.\n",
        "\n",
        "## 5. Algorithm Summary\n",
        "\n",
        "The Black Hole Strategy can be summarized as follows:\n",
        "1. Partition the graph \\( G \\) into communities using the Girvan-Newman algorithm.\n",
        "2. For each node \\( v \\in C_i \\), compute the degree centrality, betweenness centrality, and edge weight sum.\n",
        "3. Normalize the centrality metrics within each community.\n",
        "4. Calculate the gravity of each node as the average of the normalized centrality metrics.\n",
        "5. Remove nodes with the lowest gravity based on a user-defined threshold \\( T \\).\n",
        "6. Update the graph by removing the selected nodes and their edges.\n",
        "\n",
        "By applying the Black Hole Strategy, the graph is progressively sparsified while preserving its most critical nodes, allowing for more efficient analysis and processing of large-scale networks.\n"
      ],
      "metadata": {
        "id": "O6ooUdL2xeUc"
      }
    }
  ]
}